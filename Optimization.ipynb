{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dfbe9f9",
   "metadata": {},
   "source": [
    "# Post Training Pruning and Quantization of YOLOv8s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e087f93f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "554c15a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType, CalibrationDataReader, quantize_static, QuantFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b3432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process images for input into models\n",
    "# need for inference and for calibration during quantization\n",
    "def preprocess(image):\n",
    "    img = cv2.resize(image, (640, 640))\n",
    "    # normalize image\n",
    "    img = np.array(img).astype(np.float32) / 255.0\n",
    "    # reorder channels for input to model\n",
    "    img = np.transpose(img, (2,0,1))\n",
    "    # add batch dimension\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a78207",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41994cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "base_model = ultralytics.YOLO('yolov8s.pt')\n",
    "prune_unstruct_model = copy.deepcopy(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b137daf7",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a4c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune weights and biases of convolutional layers\n",
    "for name, module in prune_unstruct_model.named_modules():\n",
    "    if 'conv' in name:\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.15)\n",
    "        prune.remove(module, 'weight')\n",
    "        if module.bias is not None:\n",
    "            prune.l1_unstructured(module, name='bias', amount=0.15)\n",
    "            prune.remove(module, 'bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f492c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(prune_unstruct_model, 'models/pruned.pt')\n",
    "pruned_model = torch.load('models/pruned.pt')\n",
    "pruned_model.export(format='onnx')\n",
    "os.rename('yolov8s.onnx', 'models/pruned.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27755153",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export base model in onnx format\n",
    "base_model.export(format = 'onnx')\n",
    "os.rename('yolov8s.onnx', 'models/yolov8s.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b5577",
   "metadata": {},
   "source": [
    "### Dynamic Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bbc96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess model for quantization\n",
    "!python -m onnxruntime.quantization.preprocess --input models/yolov8s.onnx --output models/processed.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2460c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically quantize model to unsigned int8\n",
    "processed_model = 'models/processed.onnx'\n",
    "quant_model = 'models/dynamic_quantized.onnx'\n",
    "quantize_dynamic(processed_model, quant_model, weight_type=QuantType.QUInt8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff8b5d1",
   "metadata": {},
   "source": [
    "### Static Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bea73dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for getting calibration data for static quantization \n",
    "# https://quark.docs.amd.com/release-0.5.0/onnx/user_guide_datareader.html\n",
    "class DataReader(CalibrationDataReader):\n",
    "    def __init__(self, image_paths):\n",
    "        self.image_paths = image_paths\n",
    "        self.iterator = iter(self.image_paths)\n",
    "\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            image_path = next(self.iterator)\n",
    "            image = cv2.imread(image_path)\n",
    "            input_data = preprocess(image)\n",
    "            return {\"images\": input_data}\n",
    "        except StopIteration:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b37dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_set = ['data/calibrate/000000005127.jpg', 'data/calibrate/000000008447.jpg', 'data/calibrate/000000010064.jpg',\n",
    "                   'data/calibrate/000000011829.jpg', 'data/calibrate/000000016280.jpg', 'data/calibrate/000000021086.jpg',\n",
    "                   'data/calibrate/000000026680.jpg', 'data/calibrate/000000027726.jpg', 'data/calibrate/000000028308.jpg',\n",
    "                   'data/calibrate/000000029638.jpg', 'data/calibrate/000000038137.jpg', 'data/calibrate/000000038312.jpg']\n",
    "\n",
    "calibration_data_reader = DataReader(calibration_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "363d6973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static quantization of prepared model\n",
    "quantize_static(processed_model, \"models/static_quantized.onnx\",\n",
    "                weight_type=QuantType.QInt8,\n",
    "                activation_type=QuantType.QInt8,\n",
    "                calibration_data_reader=calibration_data_reader,\n",
    "                quant_format=QuantFormat.QDQ, # mixed precision quantization\n",
    "                # exclude nodes in detect head\n",
    "                nodes_to_exclude=['/model.22/Concat_3', '/model.22/Split', '/model.22/Sigmoid'\n",
    "                                 '/model.22/dfl/Reshape', '/model.22/dfl/Transpose', '/model.22/dfl/Softmax', \n",
    "                                 '/model.22/dfl/conv/Conv', '/model.22/dfl/Reshape_1', '/model.22/Slice_1',\n",
    "                                 '/model.22/Slice', '/model.22/Add_1', '/model.22/Sub', '/model.22/Div_1',\n",
    "                                  '/model.22/Concat_4', '/model.22/Mul_2', '/model.22/Concat_5'],\n",
    "                per_channel=False,\n",
    "                reduce_range=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ee598a",
   "metadata": {},
   "source": [
    "# Testing Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d43c2e8",
   "metadata": {},
   "source": [
    "### Functions for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19b822cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict for classes in COCO\n",
    "classes = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train',\n",
    "                        7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant',\n",
    "                        11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog',\n",
    "                        17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant',\n",
    "                        21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag',\n",
    "                        27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis',\n",
    "                        31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove',\n",
    "                        36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass',\n",
    "                        41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple',\n",
    "                        48: 'sandwich', 49: 'orange', 50: 'broccoli',\n",
    "                        51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch',\n",
    "                        58: 'potted plant', 59: 'bed', 60: 'dining table',\n",
    "                        61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard',\n",
    "                        67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster',\n",
    "                        71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors',\n",
    "                        77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
    "\n",
    "# Define color palette for plotting detections\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "051fe32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and initialize onnx session\n",
    "def load_model(model_path):\n",
    "    # run inference on CPU\n",
    "    session = onnxruntime.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
    "    model_inputs = session.get_inputs()\n",
    "    input_shape = model_inputs[0].shape\n",
    "    input_w, input_h = input_shape[2], input_shape[3]\n",
    "    return session, input_w, input_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "720fd1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on image (detect objects in provided image)\n",
    "def detect(session, img_data):\n",
    "    ort = onnxruntime.OrtValue.ortvalue_from_numpy(img_data)\n",
    "    results = session.run([\"output0\"], {\"images\": ort})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78f55da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draws given box and label/confidence on image \n",
    "def draw_boxes(img, box, score, class_id):\n",
    "    # top left x, top left y, width, height\n",
    "    x, y, w, h = box\n",
    "    color = colors[class_id]\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "    label = f\"{classes[class_id]}: {score:.2f}\"\n",
    "    cv2.putText(img, label, (x - 10, y - 10), cv2.FONT_HERSHEY_DUPLEX, 0.5, color, 1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d339159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocess results\n",
    "# Determines highest probability predictions, draws predictions\n",
    "# Returns image with detections and dict of detections\n",
    "# https://github.com/ultralytics/ultralytics/blob/e5cb35edfc3bbc9d7d7db8a6042778a751f0e39e/examples/YOLOv8-OpenCV-ONNX-Python/\n",
    "def postprocess(results, image, input_w, input_h, confidence=0.35, iou=0.5):\n",
    "    img_h, img_w = image.shape[:2]\n",
    "    outputs = np.transpose(np.squeeze(results[0]))\n",
    "    rows = outputs.shape[0]\n",
    "    boxes, scores, class_ids = [], [], []\n",
    "    # scales to rescale bounding boxes to image size\n",
    "    x_scale, y_scale = img_w / input_w, img_h / input_h\n",
    "\n",
    "    for i in range(rows):\n",
    "        class_scores = outputs[i][4:]\n",
    "        max_score = np.amax(class_scores)\n",
    "        if max_score >= confidence:\n",
    "            class_id = np.argmax(class_scores)\n",
    "            # bounding box\n",
    "            x, y, w, h = outputs[i][0], outputs[i][1], outputs[i][2], outputs[i][3]\n",
    "            # rescale bounding box to pixels, reformat from center to top left\n",
    "            left = int((x - w / 2) * x_scale)\n",
    "            top = int((y - h / 2) * y_scale)\n",
    "            width = int(w * x_scale)\n",
    "            height = int(h * y_scale)\n",
    "\n",
    "            boxes.append([left, top, width, height])\n",
    "            scores.append(max_score)\n",
    "            class_ids.append(class_id)\n",
    "            \n",
    "    # non maximum suppression on potential detections\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, scores, confidence, iou)\n",
    "    detections = []\n",
    "    for i in indices:\n",
    "        box = boxes[i]\n",
    "        score = scores[i]\n",
    "        class_id = class_ids[i]\n",
    "        detection = {\n",
    "            'class_id': class_ids[i],\n",
    "            'confidence': scores[i],\n",
    "            'box': box}\n",
    "        detections.append(detection)\n",
    "        # draw detection on image\n",
    "        image = draw_boxes(image, box, score, class_id)\n",
    "    return image, detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc542b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main inference function\n",
    "# Reads image, preprocesses image, detects objects, postprocesses detections\n",
    "def inference(img_path, session, input_w, input_h):\n",
    "    image = cv2.imread(img_path)\n",
    "    img_data = preprocess(image)\n",
    "    results = detect(session, img_data)\n",
    "    img_out, detections = postprocess(results, image, input_w, input_h)\n",
    "    return img_out, detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4707a17",
   "metadata": {},
   "source": [
    "### Functions for Mean Average Precision Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c8543ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dict of labels for each image\n",
    "def load_labels(label_dir):\n",
    "    labels = {}\n",
    "    for file in os.listdir(label_dir):\n",
    "        if file.endswith('.txt'):\n",
    "            image_name = file.replace('.txt', '.jpg')\n",
    "            with open(os.path.join(label_dir, file), 'r') as f:\n",
    "                boxes = []\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    class_id = int(parts[0])\n",
    "                    box = [int(pixel) for pixel in parts[1:]]\n",
    "                    boxes.append({'class_id': class_id, 'box': box})\n",
    "            labels[image_name] = boxes\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e15dca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat detections and labels structure to match input to ODMetrics\n",
    "def reformat_data(detections, labels):\n",
    "    formatted_detections = []\n",
    "    formatted_labels = []\n",
    "    \n",
    "    for image_name in detections.keys():\n",
    "        detection_data = detections[image_name]\n",
    "        label_data = labels.get(image_name, [])\n",
    "        \n",
    "        # Format detections\n",
    "        formatted_detections.append({\n",
    "            \"boxes\": [item[\"box\"] for item in detection_data],\n",
    "            \"labels\": [int(item[\"class_id\"]) for item in detection_data],\n",
    "            \"scores\": [item[\"confidence\"] for item in detection_data],\n",
    "        })\n",
    "        \n",
    "        # Format labels\n",
    "        formatted_labels.append({\n",
    "            \"boxes\": [item[\"box\"] for item in label_data],\n",
    "            \"labels\": [int(item[\"class_id\"]) for item in label_data],\n",
    "        })\n",
    "    \n",
    "    return formatted_detections, formatted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4368734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from od_metrics import ODMetrics\n",
    "\n",
    "# Calculate mean average precision for detections\n",
    "def calc_map(detections, label_dir):\n",
    "    # Load labels for all images\n",
    "    labels = load_labels(label_dir)\n",
    "\n",
    "    f_detections, f_labels = reformat_data(detections, labels)\n",
    "    metrics = ODMetrics()\n",
    "    output = metrics.compute(f_labels, f_detections)\n",
    "\n",
    "    return output['mAP@[.5 | all | 100]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8be144f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads model, runs inference and calculates statistics\n",
    "\n",
    "def evaluate_model(model_path, image_path, label_path, out_path):\n",
    "    # Load model\n",
    "    session, input_w, input_h = load_model(model_path)\n",
    "    \n",
    "    # Setup for outputs\n",
    "    model_name = os.path.splitext(os.path.basename(model_path))[0]\n",
    "    model_output_dir = os.path.join(out_path, model_name)\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get image files\n",
    "    image_files = [os.path.join(image_path, f) for f in os.listdir(image_path) if f.endswith(('.jpg', '.png'))]\n",
    "    \n",
    "    total_time = 0\n",
    "    detections = {}\n",
    "\n",
    "    # Run inference on each image\n",
    "    for img in image_files:\n",
    "        start_time = time.perf_counter()\n",
    "        img_out, dets = inference(img, session, input_w, input_h)\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        total_time += (end_time - start_time)\n",
    "        detections[os.path.basename(img)] = dets\n",
    "        \n",
    "        # Save image\n",
    "        output_file_path = os.path.join(model_output_dir, os.path.basename(img))\n",
    "        cv2.imwrite(output_file_path, img_out)\n",
    "\n",
    "    # Mean average precision\n",
    "    mean_ap = calc_map(detections, label_path)\n",
    "    \n",
    "    # Average inference time per image\n",
    "    avg_inference_time = total_time / len(image_files) if image_files else 0\n",
    "    \n",
    "    return mean_ap, total_time, avg_inference_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaea864",
   "metadata": {},
   "source": [
    "## Testing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16f3dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'data/images'\n",
    "label_path = 'data/labels'\n",
    "out_path = 'outputs'\n",
    "os.makedirs(out_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53c2497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'models/yolov8s.onnx'\n",
    "pruned_model = 'models/pruned.onnx'\n",
    "quantized_dynamic_model = 'models/dynamic_quantized.onnx'\n",
    "quantized_static_model = 'models/static_quantized.onnx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01f5bb2",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c6cb6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (mAP): 0.39466801486037867\n",
      "Total Inference Time: 16.473916871938854 seconds\n",
      "Average Inference Time: 0.13074537199951472 seconds\n"
     ]
    }
   ],
   "source": [
    "mean_ap, total_time, avg_time = evaluate_model(base_model, image_path, label_path, out_path)\n",
    "print(f\"Mean Average Precision (mAP): {mean_ap}\")\n",
    "print(f\"Total Inference Time: {total_time} seconds\")\n",
    "print(f\"Average Inference Time: {avg_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f872989",
   "metadata": {},
   "source": [
    "### Pruned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63323323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (mAP): 0.2811851637021608\n",
      "Total Inference Time: 16.180145043879747 seconds\n",
      "Average Inference Time: 0.12841384955460117 seconds\n"
     ]
    }
   ],
   "source": [
    "mean_ap, total_time, avg_time = evaluate_model(pruned_model, image_path, label_path, out_path)\n",
    "print(f\"Mean Average Precision (mAP): {mean_ap}\")\n",
    "print(f\"Total Inference Time: {total_time} seconds\")\n",
    "print(f\"Average Inference Time: {avg_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb13d4",
   "metadata": {},
   "source": [
    "### Dynamic Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3ff2a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (mAP): 0.39286253774346397\n",
      "Total Inference Time: 23.82644220686052 seconds\n",
      "Average Inference Time: 0.1890987476734962 seconds\n"
     ]
    }
   ],
   "source": [
    "mean_ap, total_time, avg_time = evaluate_model(quantized_dynamic_model, image_path, label_path, out_path)\n",
    "print(f\"Mean Average Precision (mAP): {mean_ap}\")\n",
    "print(f\"Total Inference Time: {total_time} seconds\")\n",
    "print(f\"Average Inference Time: {avg_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4461faf6",
   "metadata": {},
   "source": [
    "### Static Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d5b4e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (mAP): 0.32395280241791236\n",
      "Total Inference Time: 11.88469374878332 seconds\n",
      "Average Inference Time: 0.09432296626018508 seconds\n"
     ]
    }
   ],
   "source": [
    "mean_ap, total_time, avg_time = evaluate_model(quantized_static_model, image_path, label_path, out_path)\n",
    "print(f\"Mean Average Precision (mAP): {mean_ap}\")\n",
    "print(f\"Total Inference Time: {total_time} seconds\")\n",
    "print(f\"Average Inference Time: {avg_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
